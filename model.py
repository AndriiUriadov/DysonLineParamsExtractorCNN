# =============================================================================
# –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê –ù–ï–ô–†–û–ù–ù–û–á –ú–ï–†–ï–ñ–Ü
# =============================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class DysonianLineCNN(nn.Module):
    """
    CNN –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ª—ñ–Ω—ñ—ó –î–∞–π—Å–æ–Ω–∞
    
    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:
    - –í—Ö—ñ–¥–Ω–∏–π —à–∞—Ä: 4096 –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    - –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏ –∑ –±–∞–≥–∞—Ç–æ–≥–æ–ª–æ–≤–æ—é —É–≤–∞–≥–æ—é
    - –ü–æ–≤–Ω–æ–∑–≤'—è–∑–Ω—ñ —à–∞—Ä–∏
    - –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä: 4 –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (B0, dB, p, I)
    """
    
    def __init__(self, input_size=4096, hidden_sizes=[512, 256, 128], num_heads=8, dropout=0.2):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        
        Args:
            input_size: —Ä–æ–∑–º—ñ—Ä –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            hidden_sizes: —Å–ø–∏—Å–æ–∫ —Ä–æ–∑–º—ñ—Ä—ñ–≤ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤
            num_heads: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–ª—ñ–≤ —É multi-head attention
            dropout: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç dropout
        """
        super(DysonianLineCNN, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_heads = num_heads
        self.dropout = dropout
        
        # –ü–µ—Ä—à–∏–π —à–∞—Ä - –∑–≥–æ—Ä—Ç–∫–æ–≤–∏–π
        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(64)
        
        # Multi-head attention —à–∞—Ä
        self.attention = nn.MultiheadAttention(
            embed_dim=64, 
            num_heads=num_heads, 
            dropout=dropout,
            batch_first=True
        )
        
        # –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏
        self.conv_layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()
        
        in_channels = 64
        for hidden_size in hidden_sizes:
            self.conv_layers.append(nn.Conv1d(in_channels, hidden_size, kernel_size=5, padding=2))
            self.bn_layers.append(nn.BatchNorm1d(hidden_size))
            in_channels = hidden_size
        
        # Global Average Pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # –ü–æ–≤–Ω–æ–∑–≤'—è–∑–Ω—ñ —à–∞—Ä–∏
        self.fc_layers = nn.ModuleList()
        fc_sizes = [hidden_sizes[-1]] + [256, 128, 64]
        
        for i in range(len(fc_sizes) - 1):
            self.fc_layers.append(nn.Linear(fc_sizes[i], fc_sizes[i + 1]))
        
        # –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä
        self.output_layer = nn.Linear(fc_sizes[-1], 4)
        
        # Dropout
        self.dropout_layer = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É
        
        Args:
            x: –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ shape (batch_size, input_size)
        
        Returns:
            output: –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ shape (batch_size, 4)
        """
        batch_size = x.size(0)
        
        # Reshape –¥–ª—è –∑–≥–æ—Ä—Ç–∫–æ–≤–∏—Ö —à–∞—Ä—ñ–≤: (batch_size, 1, input_size)
        x = x.unsqueeze(1)
        
        # –ü–µ—Ä—à–∏–π –∑–≥–æ—Ä—Ç–∫–æ–≤–∏–π —à–∞—Ä
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout_layer(x)
        
        # Multi-head attention
        # Reshape –¥–ª—è attention: (batch_size, seq_len, features)
        x_att = x.transpose(1, 2)  # (batch_size, input_size, 64)
        att_output, _ = self.attention(x_att, x_att, x_att)
        x = att_output.transpose(1, 2)  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –¥–æ (batch_size, 64, input_size)
        
        # –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏
        for conv, bn in zip(self.conv_layers, self.bn_layers):
            x = F.relu(bn(conv(x)))
            x = self.dropout_layer(x)
        
        # Global Average Pooling
        x = self.global_pool(x)  # (batch_size, hidden_sizes[-1], 1)
        x = x.squeeze(-1)  # (batch_size, hidden_sizes[-1])
        
        # –ü–æ–≤–Ω–æ–∑–≤'—è–∑–Ω—ñ —à–∞—Ä–∏
        for fc in self.fc_layers:
            x = F.relu(fc(x))
            x = self.dropout_layer(x)
        
        # –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä
        output = self.output_layer(x)
        
        return output
    
    def get_model_summary(self):
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å –º–æ–¥–µ–ª—ñ
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        summary = f"""
        üß† –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–Ü DysonLineCNN:
        
        üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏:
        - –í—Ö—ñ–¥–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {self.input_size}
        - –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏: {self.hidden_sizes}
        - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–ª—ñ–≤ attention: {self.num_heads}
        - Dropout: {self.dropout}
        
        üìà –†–æ–∑–º—ñ—Ä–∏:
        - –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,}
        - –ù–∞–≤—á–∞—î–º—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: {trainable_params:,}
        
        üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
        1. –ó–≥–æ—Ä—Ç–∫–æ–≤–∏–π —à–∞—Ä (1 ‚Üí 64 –∫–∞–Ω–∞–ª–∏)
        2. Multi-head attention (8 –≥–æ–ª—ñ–≤)
        3. –ó–≥–æ—Ä—Ç–∫–æ–≤—ñ —à–∞—Ä–∏: {self.hidden_sizes}
        4. Global Average Pooling
        5. –ü–æ–≤–Ω–æ–∑–≤'—è–∑–Ω—ñ —à–∞—Ä–∏: {self.hidden_sizes[-1]} ‚Üí 256 ‚Üí 128 ‚Üí 64
        6. –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä: 64 ‚Üí 4 –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        """
        
        return summary

def create_model(input_size=4096, hidden_sizes=[512, 256, 128], num_heads=8, dropout=0.2, device=None):
    """
    –°—Ç–≤–æ—Ä—é—î —Ç–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –º–æ–¥–µ–ª—å
    
    Args:
        input_size: —Ä–æ–∑–º—ñ—Ä –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        hidden_sizes: —Å–ø–∏—Å–æ–∫ —Ä–æ–∑–º—ñ—Ä—ñ–≤ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤
        num_heads: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≥–æ–ª—ñ–≤ —É multi-head attention
        dropout: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç dropout
        device: –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å ('cuda' –∞–±–æ 'cpu'), —è–∫—â–æ None - –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è
    
    Returns:
        model: —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å
    """
    print("üèóÔ∏è –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π, —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–µ–Ω–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    model = DysonianLineCNN(
        input_size=input_size,
        hidden_sizes=hidden_sizes,
        num_heads=num_heads,
        dropout=dropout
    )
    
    # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π
    model = model.to(device)
    
    # –í–∏–≤–æ–¥–∏–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –º–æ–¥–µ–ª—å
    print(model.get_model_summary())
    
    return model

def count_parameters(model):
    """
    –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ
    
    Args:
        model: PyTorch –º–æ–¥–µ–ª—å
    
    Returns:
        tuple: (–∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å, –Ω–∞–≤—á–∞—î–º—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏)
    """
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return total_params, trainable_params

def test_model(model, input_size=4096, batch_size=4):
    """
    –¢–µ—Å—Ç—É—î –º–æ–¥–µ–ª—å –Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
    
    Args:
        model: –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        input_size: —Ä–æ–∑–º—ñ—Ä –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É
    
    Returns:
        bool: True —è–∫—â–æ —Ç–µ—Å—Ç –ø—Ä–æ–π—à–æ–≤ —É—Å–ø—ñ—à–Ω–æ
    """
    print("üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –¥–∞–Ω—ñ
        x = torch.randn(batch_size, input_size)
        
        # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –Ω–∞ —Ç–æ–π –∂–µ –ø—Ä–∏—Å—Ç—Ä—ñ–π, —â–æ –π –º–æ–¥–µ–ª—å
        device = next(model.parameters()).device
        x = x.to(device)
        
        # –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥
        with torch.no_grad():
            output = model(x)
        
        print(f"‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π—à–æ–≤ —É—Å–ø—ñ—à–Ω–æ!")
        print(f"   –í—Ö—ñ–¥–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {x.shape}")
        print(f"   –í–∏—Ö—ñ–¥–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {output.shape}")
        print(f"   –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—ñ: {str(e)}")
        return False 