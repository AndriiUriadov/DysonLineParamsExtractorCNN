# =============================================================================
# –ù–ê–í–ß–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü
# =============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import time
import os

class DysonianLineTrainer:
    """
    –ö–ª–∞—Å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ DysonianLineCNN
    """
    
    def __init__(self, model, device='cuda', learning_rate=0.001, weight_decay=1e-5):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–µ–Ω–µ—Ä–∞
        
        Args:
            model: –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
            device: –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å
            learning_rate: —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
            weight_decay: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
        """
        self.model = model
        self.device = device
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π –≤—Ç—Ä–∞—Ç
        self.criterion = nn.MSELoss()
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
        self.optimizer = optim.Adam(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        # Scheduler –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è learning rate
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.5, 
            patience=10, 
            verbose=True
        )
        
        # –Ü—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
        self.train_losses = []
        self.val_losses = []
        self.train_metrics = []
        self.val_metrics = []
        
        print(f"üéØ –¢—Ä–µ–Ω–µ—Ä —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ:")
        print(f"   –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
        print(f"   Learning rate: {learning_rate}")
        print(f"   Weight decay: {weight_decay}")
    
    def create_data_loaders(self, X_train, X_val, X_test, y_train, y_val, y_test, batch_size=16):
        """
        –°—Ç–≤–æ—Ä—é—î DataLoader –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        
        Args:
            X_train, X_val, X_test: –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
            y_train, y_val, y_test: –≤–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
            batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É (–∑–º–µ–Ω—à–µ–Ω–æ –¥–æ 16 –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ)
        
        Returns:
            tuple: (train_loader, val_loader, test_loader)
        """
        print("üì¶ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataLoader...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ PyTorch —Ç–µ–Ω–∑–æ—Ä–∏ (–Ω–∞ CPU –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ)
        X_train_tensor = torch.FloatTensor(X_train)
        X_val_tensor = torch.FloatTensor(X_val)
        X_test_tensor = torch.FloatTensor(X_test)
        
        y_train_tensor = torch.FloatTensor(y_train)
        y_val_tensor = torch.FloatTensor(y_val)
        y_test_tensor = torch.FloatTensor(y_test)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ DataLoader –∑ –º–µ–Ω—à–∏–º batch_size
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)
        
        print(f"‚úÖ DataLoader —Å—Ç–≤–æ—Ä–µ–Ω–æ:")
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
        print(f"   Batch size: {batch_size} (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è –ø–∞–º'—è—Ç—ñ)")
        
        return train_loader, val_loader, test_loader
    
    def train_epoch(self, train_loader):
        """
        –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω—ñ–π –µ–ø–æ—Å—ñ
        
        Args:
            train_loader: DataLoader –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        
        Returns:
            tuple: (—Å–µ—Ä–µ–¥–Ω—è –≤—Ç—Ä–∞—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏)
        """
        self.model.train()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ GPU
            data = data.to(self.device)
            target = target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # –ó–≤–æ—Ä–æ—Ç–Ω–∏–π –ø—Ä–æ—Ö—ñ–¥
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –º–µ—Ç—Ä–∏–∫ (–Ω–∞ CPU –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ)
            all_predictions.append(output.detach().cpu().numpy())
            all_targets.append(target.detach().cpu().numpy())
            
            # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 100 batch'—ñ–≤
            if batch_idx % 100 == 0 and batch_idx > 0:
                print(f"     [BATCH] Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.6f}")
            
            # –û—á–∏—â–∞—î–º–æ –∫–µ—à GPU
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(train_loader)
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
    
    def validate_epoch(self, val_loader):
        """
        –í–∞–ª—ñ–¥–∞—Ü—ñ—è –Ω–∞ –æ–¥–Ω—ñ–π –µ–ø–æ—Å—ñ
        
        Args:
            val_loader: DataLoader –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        
        Returns:
            tuple: (—Å–µ—Ä–µ–¥–Ω—è –≤—Ç—Ä–∞—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏)
        """
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ GPU
                data = data.to(self.device)
                target = target.to(self.device)
                
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                all_predictions.append(output.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
    
    def calculate_metrics(self, predictions, targets):
        """
        –û–±—á–∏—Å–ª—é—î –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
        
        Args:
            predictions: –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            targets: —Å–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        
        Returns:
            dict: —Å–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        param_metrics = {}
        param_names = ['B0', 'dB', 'p', 'I']
        
        for i, param in enumerate(param_names):
            param_mse = mean_squared_error(targets[:, i], predictions[:, i])
            param_rmse = np.sqrt(param_mse)
            param_r2 = r2_score(targets[:, i], predictions[:, i])
            
            param_metrics[param] = {
                'MSE': param_mse,
                'RMSE': param_rmse,
                'R2': param_r2
            }
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'R2': r2,
            'param_metrics': param_metrics
        }
    
    def train(self, train_loader, val_loader, num_epochs=100, early_stopping_patience=20):
        """
        –ü–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è
        
        Args:
            train_loader: DataLoader –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
            num_epochs: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
            early_stopping_patience: —Ç–µ—Ä–ø—ñ–Ω–Ω—è –¥–ª—è early stopping
        
        Returns:
            dict: —ñ—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        print(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {num_epochs} –µ–ø–æ—Ö...")
        
        best_val_loss = float('inf')
        patience_counter = 0
        start_time = time.time()
        
        for epoch in range(num_epochs):
            epoch_start = time.time()
            
            # –ù–∞–≤—á–∞–Ω–Ω—è
            train_loss, train_metrics = self.train_epoch(train_loader)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            val_loss, val_metrics = self.validate_epoch(val_loader)
            
            # –û–Ω–æ–≤–ª—é—î–º–æ scheduler
            self.scheduler.step(val_loss)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_metrics.append(train_metrics)
            self.val_metrics.append(val_metrics)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ best loss
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
                self.save_model('best_model.pth')
            else:
                patience_counter += 1
            
            # –í–∏–≤–æ–¥–∏–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            epoch_time = time.time() - epoch_start
            print(f"[EPOCH] Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s):")
            print(f"   [LOSS] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            print(f"   [R2] Train R¬≤: {train_metrics['R2']:.4f}, Val R¬≤: {val_metrics['R2']:.4f}")
            print(f"   [LR] Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"   [TIME] –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {(time.time() - start_time)/60:.1f} —Ö–≤")
            print("-" * 50)
            
            # Early stopping
            if patience_counter >= early_stopping_patience:
                print(f"‚èπÔ∏è  Early stopping –Ω–∞ –µ–ø–æ—Å—ñ {epoch+1}")
                break
        
        total_time = time.time() - start_time
        print(f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/60:.1f} —Ö–≤–∏–ª–∏–Ω")
        print(f"   –ù–∞–π–∫—Ä–∞—â–∞ val loss: {best_val_loss:.6f}")
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'best_val_loss': best_val_loss
        }
    
    def evaluate(self, test_loader):
        """
        –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        
        Args:
            test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        
        Returns:
            dict: –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        """
        print("üìä –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ GPU
                data = data.to(self.device)
                target = target.to(self.device)
                
                output = self.model(data)
                all_predictions.append(output.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö:")
        print(f"   MSE: {metrics['MSE']:.6f}")
        print(f"   RMSE: {metrics['RMSE']:.6f}")
        print(f"   R¬≤: {metrics['R2']:.4f}")
        
        # –í–∏–≤–æ–¥–∏–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        for param, param_metrics in metrics['param_metrics'].items():
            print(f"   {param}: R¬≤={param_metrics['R2']:.4f}, RMSE={param_metrics['RMSE']:.6f}")
        
        return metrics
    
    def save_model(self, filename):
        """
        –ó–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å
        
        Args:
            filename: –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
        """
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics
        }, filename)
        print(f"üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")
    
    def load_model(self, filename):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å
        
        Args:
            filename: –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
        """
        checkpoint = torch.load(filename, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.train_metrics = checkpoint['train_metrics']
        self.val_metrics = checkpoint['val_metrics']
        print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {filename}")
    
    def plot_training_history(self):
        """
        –í—ñ–∑—É–∞–ª—ñ–∑—É—î —ñ—Å—Ç–æ—Ä—ñ—é –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(self.train_losses, label='Train Loss')
        axes[0, 0].plot(self.val_losses, label='Val Loss')
        axes[0, 0].set_title('Loss History')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # R¬≤
        train_r2 = [m['R2'] for m in self.train_metrics]
        val_r2 = [m['R2'] for m in self.val_metrics]
        axes[0, 1].plot(train_r2, label='Train R¬≤')
        axes[0, 1].plot(val_r2, label='Val R¬≤')
        axes[0, 1].set_title('R¬≤ History')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('R¬≤')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # RMSE
        train_rmse = [m['RMSE'] for m in self.train_metrics]
        val_rmse = [m['RMSE'] for m in self.val_metrics]
        axes[1, 0].plot(train_rmse, label='Train RMSE')
        axes[1, 0].plot(val_rmse, label='Val RMSE')
        axes[1, 0].set_title('RMSE History')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('RMSE')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # Learning Rate
        lr_history = []
        for i in range(len(self.train_losses)):
            lr_history.append(self.optimizer.param_groups[0]['lr'])
        axes[1, 1].plot(lr_history)
        axes[1, 1].set_title('Learning Rate History')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def plot_loss_history(self, figsize=(12, 8)):
        """
        –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ –≤—Ç—Ä–∞—Ç, –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–∏–π –¥–æ Keras
        
        Args:
            figsize: —Ä–æ–∑–º—ñ—Ä –≥—Ä–∞—Ñ—ñ–∫–∞
        """
        plt.figure(figsize=figsize)
        
        # –ü–æ–±—É–¥–æ–≤–∞ –ª—ñ–Ω—ñ–π –≤—Ç—Ä–∞—Ç
        epochs = range(1, len(self.train_losses) + 1)
        plt.plot(epochs, self.train_losses, 'b-', linewidth=2, label='Train Loss', alpha=0.8)
        plt.plot(epochs, self.val_losses, 'r-', linewidth=2, label='Validation Loss', alpha=0.8)
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss (MSE)', fontsize=12)
        plt.title('–ó–º—ñ–Ω–∞ Loss –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        plt.tight_layout()
        plt.show()
        
        # –í–∏–≤–æ–¥–∏–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Ç—Ä–∞—Ç:")
        print(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∏–π Train Loss: {self.train_losses[0]:.6f}")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π Train Loss: {self.train_losses[-1]:.6f}")
        print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è Train Loss: {((self.train_losses[0] - self.train_losses[-1]) / self.train_losses[0] * 100):.1f}%")
        print(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∏–π Val Loss: {self.val_losses[0]:.6f}")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π Val Loss: {self.val_losses[-1]:.6f}")
        print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è Val Loss: {((self.val_losses[0] - self.val_losses[-1]) / self.val_losses[0] * 100):.1f}%")
        print(f"   –ù–∞–π–∫—Ä–∞—â–∏–π Val Loss: {min(self.val_losses):.6f} (–µ–ø–æ–∫–∞ {self.val_losses.index(min(self.val_losses)) + 1})")
    
    def plot_r2_history(self, figsize=(12, 8)):
        """
        –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ R¬≤, –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–∏–π –¥–æ Keras
        
        Args:
            figsize: —Ä–æ–∑–º—ñ—Ä –≥—Ä–∞—Ñ—ñ–∫–∞
        """
        plt.figure(figsize=figsize)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ R¬≤ –∑–Ω–∞—á–µ–Ω–Ω—è
        train_r2 = [m['R2'] for m in self.train_metrics]
        val_r2 = [m['R2'] for m in self.val_metrics]
        
        # –ü–æ–±—É–¥–æ–≤–∞ –ª—ñ–Ω—ñ–π R¬≤
        epochs = range(1, len(train_r2) + 1)
        plt.plot(epochs, train_r2, 'g-', linewidth=2, label='Train R¬≤', alpha=0.8)
        plt.plot(epochs, val_r2, 'm-', linewidth=2, label='Validation R¬≤', alpha=0.8)
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('R¬≤ Score', fontsize=12)
        plt.title('–ó–º—ñ–Ω–∞ R¬≤ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        plt.tight_layout()
        plt.show()
        
        # –í–∏–≤–æ–¥–∏–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ R¬≤:")
        print(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∏–π Train R¬≤: {train_r2[0]:.4f}")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π Train R¬≤: {train_r2[-1]:.4f}")
        print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è Train R¬≤: {((train_r2[-1] - train_r2[0]) / abs(train_r2[0]) * 100):.1f}%")
        print(f"   –ü–æ—á–∞—Ç–∫–æ–≤–∏–π Val R¬≤: {val_r2[0]:.4f}")
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∏–π Val R¬≤: {val_r2[-1]:.4f}")
        print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è Val R¬≤: {((val_r2[-1] - val_r2[0]) / abs(val_r2[0]) * 100):.1f}%")
        print(f"   –ù–∞–π–∫—Ä–∞—â–∏–π Val R¬≤: {max(val_r2):.4f} (–µ–ø–æ–∫–∞ {val_r2.index(max(val_r2)) + 1})")

def create_trainer(model, learning_rate=0.001, weight_decay=1e-5, device='cuda'):
    """
    –°—Ç–≤–æ—Ä—é—î —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –º–æ–¥–µ–ª—ñ
    
    Args:
        model: –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
        learning_rate: —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
        weight_decay: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
        device: –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å
    
    Returns:
        trainer: —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Ç—Ä–µ–Ω–µ—Ä
    """
    return DysonianLineTrainer(
        model=model,
        device=device,
        learning_rate=learning_rate,
        weight_decay=weight_decay
    ) 