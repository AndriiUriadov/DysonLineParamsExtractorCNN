# =============================================================================
# –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ò–ô –¢–†–ï–ù–ï–† –î–õ–Ø –®–í–ò–î–®–û–ì–û –ù–ê–í–ß–ê–ù–ù–Ø
# =============================================================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import time
import os

class OptimizedDysonianLineTrainer:
    """
    –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∫–ª–∞—Å –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ DysonianLineCNN
    """
    
    def __init__(self, model, device='cuda', learning_rate=0.002, weight_decay=1e-4):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞
        
        Args:
            model: –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
            device: –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å
            learning_rate: –∑–±—ñ–ª—å—à–µ–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
            weight_decay: –∑–±—ñ–ª—å—à–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
        """
        self.model = model
        self.device = device
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è PyTorch –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ–π –≤—Ç—Ä–∞—Ç
        self.criterion = nn.MSELoss()
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –∑ –±—ñ–ª—å—à–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é –Ω–∞–≤—á–∞–Ω–Ω—è
        self.optimizer = optim.AdamW(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # –ë—ñ–ª—å—à –∞–≥—Ä–µ—Å–∏–≤–Ω–∏–π scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.7,  # –ú–µ–Ω—à –∞–≥—Ä–µ—Å–∏–≤–Ω–µ –∑–º–µ–Ω—à–µ–Ω–Ω—è
            patience=5,   # –ú–µ–Ω—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
            verbose=True,
            min_lr=1e-6
        )
        
        # –Ü—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
        self.train_losses = []
        self.val_losses = []
        self.train_metrics = []
        self.val_metrics = []
        
        print(f"üöÄ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç—Ä–µ–Ω–µ—Ä —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ:")
        print(f"   –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
        print(f"   Learning rate: {learning_rate} (–∑–±—ñ–ª—å—à–µ–Ω–æ)")
        print(f"   Weight decay: {weight_decay} (–∑–±—ñ–ª—å—à–µ–Ω–æ)")
        print(f"   –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: AdamW")
        print(f"   CUDNN benchmark: —É–≤—ñ–º–∫–Ω–µ–Ω–æ")
    
    def create_data_loaders(self, X_train, X_val, X_test, y_train, y_val, y_test, batch_size=32):
        """
        –°—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ DataLoader –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        
        Args:
            X_train, X_val, X_test: –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
            y_train, y_val, y_test: –≤–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
            batch_size: –∑–±—ñ–ª—å—à–µ–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
        
        Returns:
            tuple: (train_loader, val_loader, test_loader)
        """
        print("üì¶ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏—Ö DataLoader...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ PyTorch —Ç–µ–Ω–∑–æ—Ä–∏
        X_train_tensor = torch.FloatTensor(X_train)
        X_val_tensor = torch.FloatTensor(X_val)
        X_test_tensor = torch.FloatTensor(X_test)
        
        y_train_tensor = torch.FloatTensor(y_train)
        y_val_tensor = torch.FloatTensor(y_val)
        y_test_tensor = torch.FloatTensor(y_test)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        # –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ DataLoader –∑ –±—ñ–ª—å—à–∏–º batch_size —Ç–∞ num_workers
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            pin_memory=True,
            num_workers=2,  # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
            persistent_workers=True  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ workers –º—ñ–∂ –µ–ø–æ—Ö–∞–º–∏
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            pin_memory=True,
            num_workers=2,
            persistent_workers=True
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            pin_memory=True,
            num_workers=2,
            persistent_workers=True
        )
        
        print(f"‚úÖ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ DataLoader —Å—Ç–≤–æ—Ä–µ–Ω–æ:")
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
        print(f"   Batch size: {batch_size} (–∑–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)")
        print(f"   Workers: 2 (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è)")
        
        return train_loader, val_loader, test_loader
    
    def train_epoch(self, train_loader):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω—ñ–π –µ–ø–æ—Å—ñ
        
        Args:
            train_loader: DataLoader –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        
        Returns:
            tuple: (—Å–µ—Ä–µ–¥–Ω—è –≤—Ç—Ä–∞—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏)
        """
        self.model.train()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ torch.cuda.amp –¥–ª—è mixed precision
        scaler = torch.cuda.amp.GradScaler()
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ GPU
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad(set_to_none=True)  # –®–≤–∏–¥—à–µ –æ—á–∏—â–µ–Ω–Ω—è
            
            # Mixed precision training
            with torch.cuda.amp.autocast():
                output = self.model(data)
                loss = self.criterion(output, target)
            
            # –ó–≤–æ—Ä–æ—Ç–Ω–∏–π –ø—Ä–æ—Ö—ñ–¥ –∑ mixed precision
            scaler.scale(loss).backward()
            scaler.step(self.optimizer)
            scaler.update()
            
            total_loss += loss.item()
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –º–µ—Ç—Ä–∏–∫
            all_predictions.append(output.detach().cpu().numpy())
            all_targets.append(target.detach().cpu().numpy())
            
            # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 50 batch'—ñ–≤ (–º–µ–Ω—à–µ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
            if batch_idx % 50 == 0 and batch_idx > 0:
                print(f"     [BATCH] Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.6f}")
            
            # –û—á–∏—â–∞—î–º–æ –∫–µ—à GPU —Ä—ñ–¥—à–µ
            if batch_idx % 20 == 0:
                torch.cuda.empty_cache()
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(train_loader)
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
    
    def validate_epoch(self, val_loader):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –Ω–∞ –æ–¥–Ω—ñ–π –µ–ø–æ—Å—ñ
        
        Args:
            val_loader: DataLoader –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        
        Returns:
            tuple: (—Å–µ—Ä–µ–¥–Ω—è –≤—Ç—Ä–∞—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏)
        """
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ GPU –∑ non_blocking
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                # Mixed precision inference
                with torch.cuda.amp.autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                all_predictions.append(output.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        return avg_loss, metrics
    
    def calculate_metrics(self, predictions, targets):
        """
        –û–±—á–∏—Å–ª—é—î –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
        
        Args:
            predictions: –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            targets: —Å–ø—Ä–∞–≤–∂–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        
        Returns:
            dict: —Å–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        param_metrics = {}
        param_names = ['B0', 'dB', 'p', 'I']
        
        for i, param in enumerate(param_names):
            param_mse = mean_squared_error(targets[:, i], predictions[:, i])
            param_rmse = np.sqrt(param_mse)
            param_r2 = r2_score(targets[:, i], predictions[:, i])
            
            param_metrics[param] = {
                'MSE': param_mse,
                'RMSE': param_rmse,
                'R2': param_r2
            }
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'R2': r2,
            'param_metrics': param_metrics
        }
    
    def train(self, train_loader, val_loader, num_epochs=100, early_stopping_patience=15):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è
        
        Args:
            train_loader: DataLoader –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
            num_epochs: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
            early_stopping_patience: —Ç–µ—Ä–ø—ñ–Ω–Ω—è –¥–ª—è early stopping
        
        Returns:
            dict: —ñ—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        print(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {num_epochs} –µ–ø–æ—Ö...")
        
        best_val_loss = float('inf')
        patience_counter = 0
        start_time = time.time()
        
        for epoch in range(num_epochs):
            epoch_start = time.time()
            
            # –ù–∞–≤—á–∞–Ω–Ω—è
            train_loss, train_metrics = self.train_epoch(train_loader)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            val_loss, val_metrics = self.validate_epoch(val_loader)
            
            # –û–Ω–æ–≤–ª—é—î–º–æ scheduler
            self.scheduler.step(val_loss)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_metrics.append(train_metrics)
            self.val_metrics.append(val_metrics)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ best loss
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
                self.save_model('best_model_optimized.pth')
            else:
                patience_counter += 1
            
            # –í–∏–≤–æ–¥–∏–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            epoch_time = time.time() - epoch_start
            print(f"[EPOCH] Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s):")
            print(f"   [LOSS] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            print(f"   [R2] Train R¬≤: {train_metrics['R2']:.4f}, Val R¬≤: {val_metrics['R2']:.4f}")
            print(f"   [LR] Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"   [TIME] –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {(time.time() - start_time)/60:.1f} —Ö–≤")
            print("-" * 50)
            
            # Early stopping
            if patience_counter >= early_stopping_patience:
                print(f"‚èπÔ∏è  Early stopping –Ω–∞ –µ–ø–æ—Å—ñ {epoch+1}")
                break
        
        total_time = time.time() - start_time
        print(f"‚úÖ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/60:.1f} —Ö–≤–∏–ª–∏–Ω")
        print(f"   –ù–∞–π–∫—Ä–∞—â–∞ val loss: {best_val_loss:.6f}")
        
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'best_val_loss': best_val_loss
        }
    
    def evaluate(self, test_loader):
        """
        –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        
        Args:
            test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        
        Returns:
            dict: –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        """
        print("üìä –û—Ü—ñ–Ω–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö...")
        
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                with torch.cuda.amp.autocast():
                    output = self.model(data)
                
                all_predictions.append(output.cpu().numpy())
                all_targets.append(target.cpu().numpy())
        
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        metrics = self.calculate_metrics(predictions, targets)
        
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö:")
        print(f"   MSE: {metrics['MSE']:.6f}")
        print(f"   RMSE: {metrics['RMSE']:.6f}")
        print(f"   R¬≤: {metrics['R2']:.4f}")
        
        # –í–∏–≤–æ–¥–∏–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        for param, param_metrics in metrics['param_metrics'].items():
            print(f"   {param}: R¬≤={param_metrics['R2']:.4f}, RMSE={param_metrics['RMSE']:.6f}")
        
        return metrics
    
    def save_model(self, filename):
        """
        –ó–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å
        
        Args:
            filename: –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
        """
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics
        }, filename)
        print(f"üíæ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {filename}")
    
    def load_model(self, filename):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å
        
        Args:
            filename: –Ω–∞–∑–≤–∞ —Ñ–∞–π–ª—É
        """
        checkpoint = torch.load(filename, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.train_metrics = checkpoint['train_metrics']
        self.val_metrics = checkpoint['val_metrics']
        print(f"üìÇ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {filename}")
    
    def plot_training_history(self):
        """
        –í—ñ–∑—É–∞–ª—ñ–∑—É—î —ñ—Å—Ç–æ—Ä—ñ—é –Ω–∞–≤—á–∞–Ω–Ω—è
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(self.train_losses, label='Train Loss')
        axes[0, 0].plot(self.val_losses, label='Val Loss')
        axes[0, 0].set_title('Loss History (Optimized)')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # R¬≤
        train_r2 = [m['R2'] for m in self.train_metrics]
        val_r2 = [m['R2'] for m in self.val_metrics]
        axes[0, 1].plot(train_r2, label='Train R¬≤')
        axes[0, 1].plot(val_r2, label='Val R¬≤')
        axes[0, 1].set_title('R¬≤ History (Optimized)')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('R¬≤')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # RMSE
        train_rmse = [m['RMSE'] for m in self.train_metrics]
        val_rmse = [m['RMSE'] for m in self.val_metrics]
        axes[1, 0].plot(train_rmse, label='Train RMSE')
        axes[1, 0].plot(val_rmse, label='Val RMSE')
        axes[1, 0].set_title('RMSE History (Optimized)')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('RMSE')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # Learning Rate
        lr_history = []
        for i in range(len(self.train_losses)):
            lr_history.append(self.optimizer.param_groups[0]['lr'])
        axes[1, 1].plot(lr_history)
        axes[1, 1].set_title('Learning Rate History (Optimized)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.show()

def create_optimized_trainer(model, learning_rate=0.002, weight_decay=1e-4, device='cuda'):
    """
    –°—Ç–≤–æ—Ä—é—î –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –º–æ–¥–µ–ª—ñ
    
    Args:
        model: –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
        learning_rate: –∑–±—ñ–ª—å—à–µ–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
        weight_decay: –∑–±—ñ–ª—å—à–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
        device: –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å
    
    Returns:
        trainer: —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Ç—Ä–µ–Ω–µ—Ä
    """
    return OptimizedDysonianLineTrainer(
        model=model,
        device=device,
        learning_rate=learning_rate,
        weight_decay=weight_decay
    ) 